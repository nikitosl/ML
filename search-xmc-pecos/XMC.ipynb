{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For Google Cloud\n",
    "from google.cloud import storage\n",
    "import google.auth\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('gd-gcp-healthcare-search')\n",
    "dataset_blob = bucket.blob('qa_ncbi_dataset.tsv')\n",
    "\n",
    "dataset_file_name = 'data/qa_ncbi_dataset.tsv'\n",
    "dataset_blob.download_to_filename(dataset_file_name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "FIQ4eYKs8s7z"
   },
   "source": [
    "!pip install libpecos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLLlHl93Q-Ov",
    "outputId": "8f4c0e13-42a4-4217-a989-8cc035f845a1"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wlSSLfNbA7kU"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kg_jfTKZ8xLq"
   },
   "outputs": [],
   "source": [
    "from pecos.xmc.xlinear.model import XLinearModel\n",
    "from pecos.xmc import Indexer, LabelEmbeddingFactory\n",
    "from pecos.utils import smat_util\n",
    "\n",
    "from pecos.utils import logging_util\n",
    "logging_util.setup_logging_config(level=1) # LOGGING CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYVV0_4Nmkz3",
    "outputId": "5e87042d-5f9a-49bb-fe80-9ed920b2871e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEGFQWTOeGvN",
    "tags": []
   },
   "source": [
    "### Tf-idf vectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qYkxtwsUQ6zF"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "simple_tokenizer = lambda t: re.findall('\\w+', t)\n",
    "\n",
    "# Replace all punctuation with spaces for \n",
    "# character-level tf-idf\n",
    "def character_preprocessor(text):\n",
    "    clear_text = re.sub('\\W+', ' ', text)\n",
    "    if clear_text[-1] != ' ':\n",
    "        clear_text = clear_text + ' '\n",
    "    if clear_text[0] != ' ':\n",
    "        clear_text = ' ' + clear_text\n",
    "    return clear_text\n",
    "\n",
    "\n",
    "# Tf-idf vectorizer with:\n",
    "#   - unigram word tf-idf\n",
    "#   - bigram word tf-idf\n",
    "#   - thrigram character tf-idf\n",
    "# Concatecated together\n",
    "class MultipleVectorizer:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 lowercase=True,\n",
    "                 stop_words=None,\n",
    "                 min_df=1,\n",
    "                 max_df=1.0,\n",
    "                 max_features=None,\n",
    "                 do_stemm=False):\n",
    "\n",
    "        # Create vectorizers\n",
    "        word_unigram_tfidf_vect = TfidfVectorizer(\n",
    "            analyzer='word',\n",
    "            ngram_range=(1, 1),\n",
    "            tokenizer=tokenizer,\n",
    "            lowercase=lowercase,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            max_features=max_features\n",
    "        )\n",
    "\n",
    "        word_bigram_tfidf_vect = TfidfVectorizer(\n",
    "            analyzer='word',\n",
    "            ngram_range=(2, 2),\n",
    "            tokenizer=tokenizer,\n",
    "            lowercase=lowercase,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            max_features=max_features\n",
    "        )\n",
    "\n",
    "        char_trigram_tfidf_vect = TfidfVectorizer(\n",
    "            analyzer='char_wb',\n",
    "            ngram_range=(3, 3),\n",
    "            lowercase=lowercase,\n",
    "            preprocessor=character_preprocessor,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            max_features=max_features\n",
    "        )\n",
    "        \n",
    "        self.vectorizers = [word_unigram_tfidf_vect, \n",
    "                            word_bigram_tfidf_vect, \n",
    "                            char_trigram_tfidf_vect]\n",
    "        \n",
    "        self.do_stemm = do_stemm\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    def simple_stemmer(self, texts_list):\n",
    "        result = []\n",
    "        print('Stemming:')\n",
    "        for text in tqdm(texts_list):\n",
    "            words=self.tokenizer(text)\n",
    "            words=[stemmer.stem(word) for word in words]\n",
    "            stemmed_text = ' '.join(words)\n",
    "            result.append(stemmed_text)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "\n",
    "        if self.do_stemm:\n",
    "            X = self.simple_stemmer(X)\n",
    "\n",
    "        print(\"Fitting vectorizers:\")\n",
    "        for vectorizer in tqdm(self.vectorizers):\n",
    "            vectorizer.fit(X)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        if self.do_stemm:\n",
    "            X = self.simple_stemmer(X)\n",
    "\n",
    "        print('Transforming:')\n",
    "        output = []\n",
    "        for vectorizer in tqdm(self.vectorizers):\n",
    "            output.append(vectorizer.transform(X))\n",
    "            \n",
    "        stack = hstack(output)\n",
    "        csr_matrix = scipy.sparse.csr_matrix(stack, dtype=np.float32)\n",
    "\n",
    "        return csr_matrix\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        output = []\n",
    "        for vectorizer in self.vectorizers:\n",
    "            output.append(vectorizer.get_feature_names_out())\n",
    "        return np.hstack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDnuqkP1Q6zH",
    "outputId": "ed010bc7-13bf-4198-b894-46a25ef9a92a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:00<00:00, 606.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 854.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['a', 'ab', 'aba', 'abab', 'abb', 'b', 'ba', 'bab', 'baba', 'a a',\n",
       "       'a ba', 'a bab', 'ab a', 'ab ab', 'ab aba', 'ab abab', 'ab abb',\n",
       "       'ab ba', 'aba aba', 'aba abab', 'aba b', 'aba bab', 'abab ab',\n",
       "       'abab ba', 'abab bab', 'abb ab', 'abb b', 'b bab', 'ba bab',\n",
       "       'ba baba', 'bab ab', 'bab aba', 'bab abab', 'bab bab', 'baba ab',\n",
       "       'baba aba', 'baba baba', ' a ', ' ab', ' b ', ' ba', 'ab ', 'aba',\n",
       "       'abb', 'ba ', 'bab', 'bb '], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tf-idf vectorizer\n",
    "\n",
    "# Create vectorizer from multiple vectorizers\n",
    "multiple_vect = MultipleVectorizer(\n",
    "  lowercase=True,\n",
    "  min_df=1,\n",
    "  max_df=1.0,\n",
    "  max_features=None,\n",
    "  tokenizer=simple_tokenizer,\n",
    "  do_stemm=False\n",
    ")\n",
    "\n",
    "X = ['''ab,    abb ab!!... ba baba \n",
    "aba aba abab! bab bab\n",
    "  abab ab ab ab ab ab abb b bab ab ab ab ab ab''',\n",
    "     'ab ab, abab ab, ab: abab ba baba baba ab aba. bab aba',\n",
    "     'a a bab ab   abab ab ab abab, ab ab ab ab abab ab aba b',\n",
    "     'ab, ab ab ab, ab ab ab   ba bab ab a, bab ab! ab ab ab a ba']\n",
    "result = multiple_vect.fit_transform(X)\n",
    "multiple_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNc4msLBeXJ_"
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuGXILWiQ6zD",
    "outputId": "6590cef1-618e-40f1-fcb2-58381dc2de69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29319, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filename = None\n",
    "try:\n",
    "    credentials, project = google.auth.default()\n",
    "    dataset_filename = dataset_file_name # GCP\n",
    "except:\n",
    "    if 'google.colab' in sys.modules:\n",
    "        dataset_filename = '/content/drive/MyDrive/qa_ncbi_dataset.tsv' # google colab\n",
    "    else:\n",
    "        dataset_filename = 'data/qa_ncbi_dataset.tsv' # local run\n",
    "        \n",
    "df = pd.read_csv(dataset_filename, sep='\\t')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBjKwwSk6WEh",
    "outputId": "3d5803a5-cdbe-4c8e-dd06-7ca16fa34cdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4040"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.question_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejYlTf03psjW"
   },
   "source": [
    "### Split into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "LrFJjA_qpsjX",
    "outputId": "66898fe0-6da0-4243-b703-75715da04851"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snippet_url</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19942</th>\n",
       "      <td>http://www.ncbi.nlm.nih.gov/pubmed/25218447</td>\n",
       "      <td>Uncovering global SUMOylation signaling networ...</td>\n",
       "      <td>SUMOylation is a reversible post-translational...</td>\n",
       "      <td>Uncovering global SUMOylation signaling networ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          snippet_url  \\\n",
       "paper_id                                                \n",
       "19942     http://www.ncbi.nlm.nih.gov/pubmed/25218447   \n",
       "\n",
       "                                                      title  \\\n",
       "paper_id                                                      \n",
       "19942     Uncovering global SUMOylation signaling networ...   \n",
       "\n",
       "                                                   abstract  \\\n",
       "paper_id                                                      \n",
       "19942     SUMOylation is a reversible post-translational...   \n",
       "\n",
       "                                             title_abstract  \n",
       "paper_id                                                     \n",
       "19942     Uncovering global SUMOylation signaling networ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paper DF # Z\n",
    "papers_df = df[['snippet_url', 'title', 'abstract']] \\\n",
    "    .drop_duplicates() \\\n",
    "    .reset_index(drop=True).reset_index() \\\n",
    "    .rename(columns={'index': 'paper_id'})\n",
    "papers_df['title_abstract'] = papers_df['title'] + '. ' + papers_df['abstract']\n",
    "\n",
    "df = df.merge(papers_df[['paper_id', 'snippet_url']])\n",
    "papers_df = papers_df.set_index('paper_id')\n",
    "\n",
    "# # Create paper id from url\n",
    "# df['paper_id'] = df.snippet_id.str.split('_')\n",
    "# df['paper_id'] = df['paper_id'].apply(lambda a: a[0]).astype(int)\n",
    "papers_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "zt2hcqWFtcbx",
    "outputId": "f1151f9e-3811-41e1-b436-d79d4a4b505d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>paper_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27794</th>\n",
       "      <th>27795</th>\n",
       "      <th>27796</th>\n",
       "      <th>27797</th>\n",
       "      <th>27798</th>\n",
       "      <th>27799</th>\n",
       "      <th>27800</th>\n",
       "      <th>27801</th>\n",
       "      <th>27802</th>\n",
       "      <th>27803</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>533aaab6d6d3ac6a34000062</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27804 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "paper_id                  0      1      2      3      4      5      6      \\\n",
       "question_id                                                                 \n",
       "533aaab6d6d3ac6a34000062      0      0      0      0      0      0      0   \n",
       "\n",
       "paper_id                  7      8      9      ...  27794  27795  27796  \\\n",
       "question_id                                    ...                        \n",
       "533aaab6d6d3ac6a34000062      0      0      0  ...      0      0      0   \n",
       "\n",
       "paper_id                  27797  27798  27799  27800  27801  27802  27803  \n",
       "question_id                                                                \n",
       "533aaab6d6d3ac6a34000062      0      0      0      0      0      0      0  \n",
       "\n",
       "[1 rows x 27804 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question-paper matrix # Y\n",
    "question_paper_df = pd.crosstab(df['question_id'], df['paper_id']).astype(int)\n",
    "question_paper_df = question_paper_df[sorted(question_paper_df.columns)]\n",
    "question_paper_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "GCNFHsvHpsja",
    "outputId": "ad4e8ae2-d6ed-41fc-ffa4-8502e3fd536e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56c71cb65795f9a73e00000b</th>\n",
       "      <td>Can DNA intercalators function as topoisomeras...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              question_text\n",
       "question_id                                                                \n",
       "56c71cb65795f9a73e00000b  Can DNA intercalators function as topoisomeras..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question df # X\n",
    "questions_df = df[['question_id', 'question_text']] \\\n",
    "    .drop_duplicates('question_id') \\\n",
    "    .set_index('question_id').loc[question_paper_df.index]\n",
    "questions_df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYlwX0SRoQfl",
    "tags": []
   },
   "source": [
    "### Split into dataframes [with splitting into sentences] (not finished)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "r9cvl2ehpB7Y"
   },
   "source": [
    "# Paper DF # Z\n",
    "senteces_df = df[['snippet_url', 'title', 'abstract']] \\\n",
    "    .drop_duplicates() \\\n",
    "    .reset_index(drop=True)\n",
    "senteces_df['title_abstract'] = senteces_df['title'] + '. ' + senteces_df['abstract']\n",
    "\n",
    "# Split paper into sentences\n",
    "text2sent_spliter = lambda t: re.split('\\.|\\!|\\?|\\n|\\t', t)\n",
    "senteces_df['paper_sentence'] = senteces_df['title_abstract'].apply(text2sent_spliter)\n",
    "\n",
    "# Dataset with sentences\n",
    "senteces_df = pd.DataFrame({'snippet_url': np.repeat(senteces_df.snippet_url.values, \n",
    "                                                     senteces_df.paper_sentence.str.len()),\n",
    "                            'sentence': np.concatenate(senteces_df.paper_sentence.values)})\n",
    "\n",
    "# Create sentence index\n",
    "senteces_df['sentence_id'] = senteces_df \\\n",
    "    .groupby('snippet_url') \\\n",
    "    .cumcount()\n",
    "\n",
    "# Paper id\n",
    "senteces_df['paper_id'] = senteces_df.snippet_url.str.split('/')\n",
    "senteces_df['paper_id'] = senteces_df['paper_id'].apply(lambda a: a[-1]).astype(int)\n",
    "senteces_df['sentence_id'] = senteces_df['paper_id'].astype(str) + '_' + senteces_df['sentence_id'].astype(str)\n",
    "\n",
    "# Merge with questions\n",
    "df_ = pd.merge(df[['question_id', 'question_text', 'snippet_url']],\n",
    "               senteces_df[['snippet_url', 'sentence_id', 'sentence']])\n",
    "\n",
    "senteces_df = senteces_df.sort_values('sentence_id') \\\n",
    "    .set_index(['sentence_id'])[['sentence']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "So-_Z9mF3wsb"
   },
   "source": [
    "senteces_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "6kvtYZ60T8c-"
   },
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "5Qyo8uLUUoqJ"
   },
   "source": [
    "df_.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "6DR-aPseoc5O"
   },
   "source": [
    "# question-paper matrix # Y\n",
    "question_paper_df = pd.crosstab(df_['question_id'], df_['sentence_id']).astype(int)\n",
    "question_paper_df = question_paper_df[sorted(question_paper_df.columns)]\n",
    "question_paper_df.sample()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2q3ku7croexU"
   },
   "source": [
    "# Question df # X\n",
    "questions_df = df[['question_id', 'question_text']] \\\n",
    "    .drop_duplicates('question_id') \\\n",
    "    .set_index('question_id').loc[question_paper_df.index]\n",
    "questions_df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab-ejPbxpsjb"
   },
   "source": [
    "### Train-test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0bzgGdzpsjb",
    "outputId": "7c8bb5a9-e653-4bef-b03d-1684067b1dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3636\n",
      "Valid: 404\n"
     ]
    }
   ],
   "source": [
    "train_question_ids, valid_question_ids = train_test_split(questions_df.index, \n",
    "                                                          test_size=0.1,\n",
    "                                                          random_state=5)\n",
    "print(f'Train: {len(train_question_ids)}')\n",
    "print(f'Valid: {len(valid_question_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "S7sQkwGUpsjc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Questions splitting\n",
    "train_questions_df = questions_df.loc[train_question_ids]\n",
    "valid_questions_df = questions_df.loc[valid_question_ids]\n",
    "\n",
    "# Question-Paper matrix splitting\n",
    "train_question_paper_df = question_paper_df.loc[train_question_ids]\n",
    "train_question_paper_csr = scipy.sparse.csr_matrix(train_question_paper_df.values, dtype=np.float32)\n",
    "valid_question_paper_df = question_paper_df.loc[valid_question_ids]\n",
    "valid_question_paper_csr = scipy.sparse.csr_matrix(valid_question_paper_df.values, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g9IMdg7psjd",
    "tags": []
   },
   "source": [
    "### Tf-idf vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYjtfjoHpsje",
    "outputId": "9666b4f0-a405-4de7-f874-3d49f16b03bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:43<00:00, 14.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 434546\n"
     ]
    }
   ],
   "source": [
    "paper_column = 'title_abstract' # title_abstract / title / abstract\n",
    "\n",
    "# Concatenate all texts\n",
    "all_texts = np.concatenate([papers_df[paper_column].values, \n",
    "                            train_questions_df.question_text.values])\n",
    "\n",
    "vectorizer = MultipleVectorizer(\n",
    "    tokenizer=simple_tokenizer,\n",
    "    lowercase=True,\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    do_stemm=False,\n",
    "    max_features=300_000\n",
    ")\n",
    "\n",
    "vectorizer.fit(all_texts)\n",
    "print(f'Vector size: {len(vectorizer.get_feature_names_out())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWu8rFkdpsje",
    "outputId": "b5660c9b-1bd8-43fc-9c54-fc085a1af9d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 79.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:37<00:00, 12.63s/it]\n"
     ]
    }
   ],
   "source": [
    "train_questions_csr = vectorizer.transform(train_questions_df.question_text.values)\n",
    "valid_questions_csr = vectorizer.transform(valid_questions_df.question_text.values)\n",
    "papers_csr = vectorizer.transform(papers_df[paper_column].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hyperparameter tuning with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_column = 'title_abstract' # title_abstract / title / abstract\n",
    "\n",
    "# Concatenate all texts\n",
    "all_texts = np.concatenate([papers_df[paper_column].values, \n",
    "                            train_questions_df.question_text.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_function(min_df,\n",
    "                       max_df,\n",
    "                       max_features,\n",
    "                       threshold, \n",
    "                       nr_splits, \n",
    "                       Cp, \n",
    "                       Cn, \n",
    "                       max_iter,\n",
    "                       eps,\n",
    "                       bias):\n",
    "    \n",
    "    max_features = int(max_features)\n",
    "    min_df = int(min_df)\n",
    "    nr_splits = int(nr_splits)\n",
    "    max_iter = int(max_iter)\n",
    "    \n",
    "    # Vectorization\n",
    "    vectorizer = MultipleVectorizer(\n",
    "        tokenizer=simple_tokenizer,\n",
    "        lowercase=True,\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        do_stemm=False,\n",
    "        max_features=max_features\n",
    "    )\n",
    "\n",
    "    vectorizer.fit(all_texts)\n",
    "    print(f'Vector size: {len(vectorizer.get_feature_names_out())}')\n",
    "    train_questions_csr = vectorizer.transform(train_questions_df.question_text.values)\n",
    "    valid_questions_csr = vectorizer.transform(valid_questions_df.question_text.values)\n",
    "    papers_csr = vectorizer.transform(papers_df[paper_column].values)\n",
    "    \n",
    "    # Train\n",
    "    hlm_args_dict = {\n",
    "        'neg_mining_chain': \"tfn\",\n",
    "        'model_chain': {\n",
    "            'threshold': threshold,\n",
    "            'max_nonzeros_per_label': None,\n",
    "            'solver_type': \"L2R_L2LOSS_SVC_DUAL\",\n",
    "            'Cp': Cp,\n",
    "            'Cn': Cn,\n",
    "            'max_iter': max_iter,\n",
    "            'eps': eps,\n",
    "            'bias': bias,\n",
    "            'threads': 8,\n",
    "            'verbose': 0,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    train_params_dict = {\n",
    "        'mode': \"full-model\",\n",
    "        'ranker_level': 1,\n",
    "        'nr_splits': nr_splits,\n",
    "        'min_codes': None,\n",
    "        'shallow': False,\n",
    "        'rel_mode': \"disable\",\n",
    "        'rel_norm': \"no-norm\",\n",
    "        'hlm_args': hlm_args_dict\n",
    "    }\n",
    "    \n",
    "    predict_params_dict = {'hlm_args': hlm_args_dict}\n",
    "    \n",
    "    train_params = XLinearModel.TrainParams.from_dict(train_params_dict, recursive=True)\n",
    "    pred_params = XLinearModel.PredParams.from_dict(predict_params_dict, recursive=True)\n",
    "\n",
    "    pifa_concat = LabelEmbeddingFactory.create(train_question_paper_csr, \n",
    "                                           train_questions_csr, \n",
    "                                           Z=papers_csr, \n",
    "                                           method=\"pifa_lf_concat\")\n",
    "\n",
    "    cluster_chain = Indexer.gen(pifa_concat, \n",
    "                                indexer_type=\"hierarchicalkmeans\",\n",
    "                                nr_splits=nr_splits)\n",
    "    \n",
    "    xlm = XLinearModel.train(train_questions_csr, \n",
    "                             train_question_paper_csr, \n",
    "                             C=cluster_chain,\n",
    "                             train_params=train_params)\n",
    "    \n",
    "    predicted_question_paper_csr = xlm.predict(valid_questions_csr,\n",
    "                                               beam_size=10,\n",
    "                                               only_topk=10)\n",
    "    \n",
    "    metric = smat_util.Metrics.generate(valid_question_paper_csr, \n",
    "                                        predicted_question_paper_csr, \n",
    "                                        topk=10)    \n",
    "    prec = metric.prec[0] # Take first precision\n",
    "    \n",
    "    return prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'min_df':(1, 10),\n",
    "    'max_df': (0, 1),\n",
    "    'max_features': (10_000, 500_000),\n",
    "    'threshold': (0.1, 1),\n",
    "    'nr_splits': (2, 1024),\n",
    "    'Cp': (0, 1),\n",
    "    'Cn': (0, 1),\n",
    "    'max_iter': (5, 500),\n",
    "    'eps': (0, 1),\n",
    "    'bias': (0, 0.5)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(f=black_box_function,\n",
    "                                 pbounds=pbounds, \n",
    "                                 verbose=2,\n",
    "                                 random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    Cn     |    Cp     |   bias    |    eps    |  max_df   | max_fe... | max_iter  |  min_df   | nr_splits | threshold |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:51<00:00, 17.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 189128\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 83.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.01733 \u001b[0m | \u001b[0m 0.222   \u001b[0m | \u001b[0m 0.8707  \u001b[0m | \u001b[0m 0.1034  \u001b[0m | \u001b[0m 0.9186  \u001b[0m | \u001b[0m 0.4884  \u001b[0m | \u001b[0m 3.098e+0\u001b[0m | \u001b[0m 384.1   \u001b[0m | \u001b[0m 5.666   \u001b[0m | \u001b[0m 305.3   \u001b[0m | \u001b[0m 0.2689  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:52<00:00, 17.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 203932\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 71.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:42<00:00, 14.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.08074 \u001b[0m | \u001b[0m 0.7384  \u001b[0m | \u001b[0m 0.2207  \u001b[0m | \u001b[0m 0.1583  \u001b[0m | \u001b[0m 0.8799  \u001b[0m | \u001b[0m 1.443e+0\u001b[0m | \u001b[0m 210.0   \u001b[0m | \u001b[0m 3.665   \u001b[0m | \u001b[0m 644.6   \u001b[0m | \u001b[0m 0.6219  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:51<00:00, 17.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 110231\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 72.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.02723 \u001b[0m | \u001b[95m 0.5999  \u001b[0m | \u001b[95m 0.2658  \u001b[0m | \u001b[95m 0.1423  \u001b[0m | \u001b[95m 0.2536  \u001b[0m | \u001b[95m 0.3276  \u001b[0m | \u001b[95m 8.064e+0\u001b[0m | \u001b[95m 86.98   \u001b[0m | \u001b[95m 9.675   \u001b[0m | \u001b[95m 983.4   \u001b[0m | \u001b[95m 0.2696  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:50<00:00, 16.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 185552\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 87.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:37<00:00, 12.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.02431 \u001b[0m | \u001b[0m 0.2046  \u001b[0m | \u001b[0m 0.3499  \u001b[0m | \u001b[0m 0.7795  \u001b[0m | \u001b[0m 0.02293 \u001b[0m | \u001b[0m 2.931e+0\u001b[0m | \u001b[0m 5.813   \u001b[0m | \u001b[0m 5.639   \u001b[0m | \u001b[0m 655.9   \u001b[0m | \u001b[0m 0.9871  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:52<00:00, 17.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 216127\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 95.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:35<00:00, 11.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.02228 \u001b[0m | \u001b[0m 0.2591  \u001b[0m | \u001b[0m 0.8025  \u001b[0m | \u001b[0m 0.4352  \u001b[0m | \u001b[0m 0.9227  \u001b[0m | \u001b[0m 0.002214\u001b[0m | \u001b[0m 2.4e+05 \u001b[0m | \u001b[0m 490.8   \u001b[0m | \u001b[0m 4.591   \u001b[0m | \u001b[0m 833.6   \u001b[0m | \u001b[0m 0.5918  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:52<00:00, 17.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 158226\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 81.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.7709  \u001b[0m | \u001b[0m 0.4849  \u001b[0m | \u001b[0m 0.01456 \u001b[0m | \u001b[0m 0.08653 \u001b[0m | \u001b[0m 0.1115  \u001b[0m | \u001b[0m 1.331e+0\u001b[0m | \u001b[0m 482.6   \u001b[0m | \u001b[0m 6.686   \u001b[0m | \u001b[0m 836.6   \u001b[0m | \u001b[0m 0.6095  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:50<00:00, 16.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 157179\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 80.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.6354  \u001b[0m | \u001b[0m 0.8119  \u001b[0m | \u001b[0m 0.4633  \u001b[0m | \u001b[0m 0.9126  \u001b[0m | \u001b[0m 0.8248  \u001b[0m | \u001b[0m 5.616e+0\u001b[0m | \u001b[0m 183.7   \u001b[0m | \u001b[0m 1.32    \u001b[0m | \u001b[0m 560.4   \u001b[0m | \u001b[0m 0.8165  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:51<00:00, 17.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 189294\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 81.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.05114 \u001b[0m | \u001b[0m 0.1887  \u001b[0m | \u001b[0m 0.1827  \u001b[0m | \u001b[0m 0.2443  \u001b[0m | \u001b[0m 0.7951  \u001b[0m | \u001b[0m 1.825e+0\u001b[0m | \u001b[0m 321.2   \u001b[0m | \u001b[0m 5.441   \u001b[0m | \u001b[0m 598.3   \u001b[0m | \u001b[0m 0.9454  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:51<00:00, 17.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 109208\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:37<00:00, 12.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.05941 \u001b[0m | \u001b[95m 0.9435  \u001b[0m | \u001b[95m 0.1117  \u001b[0m | \u001b[95m 0.4218  \u001b[0m | \u001b[95m 0.346   \u001b[0m | \u001b[95m 0.1008  \u001b[0m | \u001b[95m 1.979e+0\u001b[0m | \u001b[95m 257.6   \u001b[0m | \u001b[95m 9.65    \u001b[0m | \u001b[95m 381.7   \u001b[0m | \u001b[95m 0.1111  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:51<00:00, 17.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 363198\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 71.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.8597  \u001b[0m | \u001b[0m 0.1111  \u001b[0m | \u001b[0m 0.2392  \u001b[0m | \u001b[0m 0.85    \u001b[0m | \u001b[0m 0.5147  \u001b[0m | \u001b[0m 2.288e+0\u001b[0m | \u001b[0m 401.2   \u001b[0m | \u001b[0m 1.184   \u001b[0m | \u001b[0m 587.2   \u001b[0m | \u001b[0m 0.4702  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 258254\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 88.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.00495 \u001b[0m | \u001b[0m 0.9307  \u001b[0m | \u001b[0m 0.8253  \u001b[0m | \u001b[0m 0.1876  \u001b[0m | \u001b[0m 0.5961  \u001b[0m | \u001b[0m 0.105   \u001b[0m | \u001b[0m 2e+05   \u001b[0m | \u001b[0m 431.2   \u001b[0m | \u001b[0m 3.097   \u001b[0m | \u001b[0m 531.2   \u001b[0m | \u001b[0m 0.6247  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 317212\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.02228 \u001b[0m | \u001b[0m 0.9036  \u001b[0m | \u001b[0m 0.8835  \u001b[0m | \u001b[0m 0.3999  \u001b[0m | \u001b[0m 0.2919  \u001b[0m | \u001b[0m 0.2191  \u001b[0m | \u001b[0m 4.998e+0\u001b[0m | \u001b[0m 34.15   \u001b[0m | \u001b[0m 3.219   \u001b[0m | \u001b[0m 751.0   \u001b[0m | \u001b[0m 0.3634  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 138388\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 77.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.7812  \u001b[0m | \u001b[0m 0.5141  \u001b[0m | \u001b[0m 0.463   \u001b[0m | \u001b[0m 0.4526  \u001b[0m | \u001b[0m 0.7233  \u001b[0m | \u001b[0m 1.055e+0\u001b[0m | \u001b[0m 410.9   \u001b[0m | \u001b[0m 7.779   \u001b[0m | \u001b[0m 412.3   \u001b[0m | \u001b[0m 0.6288  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 488158\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 70.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:43<00:00, 14.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.009901\u001b[0m | \u001b[0m 0.194   \u001b[0m | \u001b[0m 0.6897  \u001b[0m | \u001b[0m 0.03151 \u001b[0m | \u001b[0m 0.1883  \u001b[0m | \u001b[0m 0.7286  \u001b[0m | \u001b[0m 4.125e+0\u001b[0m | \u001b[0m 106.2   \u001b[0m | \u001b[0m 2.211   \u001b[0m | \u001b[0m 831.8   \u001b[0m | \u001b[0m 0.3967  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 138182\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 82.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.05198 \u001b[0m | \u001b[0m 0.1123  \u001b[0m | \u001b[0m 0.4253  \u001b[0m | \u001b[0m 0.3941  \u001b[0m | \u001b[0m 0.363   \u001b[0m | \u001b[0m 0.4275  \u001b[0m | \u001b[0m 3.137e+0\u001b[0m | \u001b[0m 159.3   \u001b[0m | \u001b[0m 7.075   \u001b[0m | \u001b[0m 764.9   \u001b[0m | \u001b[0m 0.152   \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 159549\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 80.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.007426\u001b[0m | \u001b[0m 0.8647  \u001b[0m | \u001b[0m 0.635   \u001b[0m | \u001b[0m 0.02159 \u001b[0m | \u001b[0m 0.05    \u001b[0m | \u001b[0m 0.8914  \u001b[0m | \u001b[0m 4.14e+05\u001b[0m | \u001b[0m 51.33   \u001b[0m | \u001b[0m 6.378   \u001b[0m | \u001b[0m 927.6   \u001b[0m | \u001b[0m 0.5692  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 189328\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 79.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.00495 \u001b[0m | \u001b[0m 0.219   \u001b[0m | \u001b[0m 0.6041  \u001b[0m | \u001b[0m 0.2122  \u001b[0m | \u001b[0m 0.391   \u001b[0m | \u001b[0m 0.9039  \u001b[0m | \u001b[0m 1.979e+0\u001b[0m | \u001b[0m 446.2   \u001b[0m | \u001b[0m 5.388   \u001b[0m | \u001b[0m 473.8   \u001b[0m | \u001b[0m 0.4846  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 413827\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 74.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.9777  \u001b[0m | \u001b[0m 0.3306  \u001b[0m | \u001b[0m 0.3274  \u001b[0m | \u001b[0m 0.4235  \u001b[0m | \u001b[0m 0.3183  \u001b[0m | \u001b[0m 3.386e+0\u001b[0m | \u001b[0m 204.3   \u001b[0m | \u001b[0m 2.897   \u001b[0m | \u001b[0m 725.6   \u001b[0m | \u001b[0m 0.8106  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 120465\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.01238 \u001b[0m | \u001b[0m 0.9495  \u001b[0m | \u001b[0m 0.8253  \u001b[0m | \u001b[0m 0.127   \u001b[0m | \u001b[0m 0.01165 \u001b[0m | \u001b[0m 0.6251  \u001b[0m | \u001b[0m 8.204e+0\u001b[0m | \u001b[0m 208.6   \u001b[0m | \u001b[0m 7.155   \u001b[0m | \u001b[0m 32.89   \u001b[0m | \u001b[0m 0.4197  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 166294\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 70.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.4553  \u001b[0m | \u001b[0m 0.9105  \u001b[0m | \u001b[0m 0.281   \u001b[0m | \u001b[0m 0.9788  \u001b[0m | \u001b[0m 0.4408  \u001b[0m | \u001b[0m 9.083e+0\u001b[0m | \u001b[0m 417.3   \u001b[0m | \u001b[0m 2.79    \u001b[0m | \u001b[0m 492.9   \u001b[0m | \u001b[0m 0.9828  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 235700\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 76.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.02723 \u001b[0m | \u001b[0m 0.5686  \u001b[0m | \u001b[0m 0.7096  \u001b[0m | \u001b[0m 0.4922  \u001b[0m | \u001b[0m 0.877   \u001b[0m | \u001b[0m 0.6659  \u001b[0m | \u001b[0m 1.891e+0\u001b[0m | \u001b[0m 495.7   \u001b[0m | \u001b[0m 4.975   \u001b[0m | \u001b[0m 177.5   \u001b[0m | \u001b[0m 0.3097  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 17.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 317352\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 75.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.009901\u001b[0m | \u001b[0m 0.1708  \u001b[0m | \u001b[0m 0.9102  \u001b[0m | \u001b[0m 0.02456 \u001b[0m | \u001b[0m 0.2075  \u001b[0m | \u001b[0m 0.2601  \u001b[0m | \u001b[0m 3.843e+0\u001b[0m | \u001b[0m 329.5   \u001b[0m | \u001b[0m 3.123   \u001b[0m | \u001b[0m 737.4   \u001b[0m | \u001b[0m 0.4909  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 317730\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 80.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.655   \u001b[0m | \u001b[0m 0.9383  \u001b[0m | \u001b[0m 0.1345  \u001b[0m | \u001b[0m 0.7522  \u001b[0m | \u001b[0m 0.4754  \u001b[0m | \u001b[0m 4.759e+0\u001b[0m | \u001b[0m 245.0   \u001b[0m | \u001b[0m 3.743   \u001b[0m | \u001b[0m 916.7   \u001b[0m | \u001b[0m 0.5791  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 109793\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 85.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.00495 \u001b[0m | \u001b[0m 0.866   \u001b[0m | \u001b[0m 0.9781  \u001b[0m | \u001b[0m 0.02728 \u001b[0m | \u001b[0m 0.5877  \u001b[0m | \u001b[0m 0.1871  \u001b[0m | \u001b[0m 3.203e+0\u001b[0m | \u001b[0m 88.07   \u001b[0m | \u001b[0m 9.202   \u001b[0m | \u001b[0m 703.7   \u001b[0m | \u001b[0m 0.5992  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 76464\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 83.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.01485 \u001b[0m | \u001b[0m 0.3418  \u001b[0m | \u001b[0m 0.5134  \u001b[0m | \u001b[0m 0.09615 \u001b[0m | \u001b[0m 0.8192  \u001b[0m | \u001b[0m 0.1946  \u001b[0m | \u001b[0m 3.564e+0\u001b[0m | \u001b[0m 191.8   \u001b[0m | \u001b[0m 6.114   \u001b[0m | \u001b[0m 584.0   \u001b[0m | \u001b[0m 0.4331  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 138391\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 77.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.4689  \u001b[0m | \u001b[0m 0.4065  \u001b[0m | \u001b[0m 0.2511  \u001b[0m | \u001b[0m 0.8352  \u001b[0m | \u001b[0m 0.7363  \u001b[0m | \u001b[0m 1.288e+0\u001b[0m | \u001b[0m 241.9   \u001b[0m | \u001b[0m 7.712   \u001b[0m | \u001b[0m 564.1   \u001b[0m | \u001b[0m 0.8173  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:51<00:00, 17.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 502433\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 70.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.2241  \u001b[0m | \u001b[0m 0.1985  \u001b[0m | \u001b[0m 0.04302 \u001b[0m | \u001b[0m 0.6978  \u001b[0m | \u001b[0m 0.3989  \u001b[0m | \u001b[0m 3.682e+0\u001b[0m | \u001b[0m 322.1   \u001b[0m | \u001b[0m 1.19    \u001b[0m | \u001b[0m 233.2   \u001b[0m | \u001b[0m 0.6935  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 235416\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 67.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.2638  \u001b[0m | \u001b[0m 0.3416  \u001b[0m | \u001b[0m 0.2104  \u001b[0m | \u001b[0m 0.8158  \u001b[0m | \u001b[0m 0.3657  \u001b[0m | \u001b[0m 2.34e+05\u001b[0m | \u001b[0m 199.6   \u001b[0m | \u001b[0m 4.693   \u001b[0m | \u001b[0m 734.5   \u001b[0m | \u001b[0m 0.86    \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 138427\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 85.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.0495  \u001b[0m | \u001b[0m 0.6248  \u001b[0m | \u001b[0m 0.5432  \u001b[0m | \u001b[0m 0.124   \u001b[0m | \u001b[0m 0.1297  \u001b[0m | \u001b[0m 0.8449  \u001b[0m | \u001b[0m 3.365e+0\u001b[0m | \u001b[0m 31.32   \u001b[0m | \u001b[0m 7.376   \u001b[0m | \u001b[0m 848.3   \u001b[0m | \u001b[0m 0.2056  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 392460\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 70.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.6216  \u001b[0m | \u001b[0m 0.9883  \u001b[0m | \u001b[0m 0.1214  \u001b[0m | \u001b[0m 0.3285  \u001b[0m | \u001b[0m 0.7079  \u001b[0m | \u001b[0m 3.168e+0\u001b[0m | \u001b[0m 239.7   \u001b[0m | \u001b[0m 2.707   \u001b[0m | \u001b[0m 428.9   \u001b[0m | \u001b[0m 0.7382  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 570055\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 72.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:42<00:00, 14.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1557  \u001b[0m | \u001b[0m 0.0277  \u001b[0m | \u001b[0m 0.1311  \u001b[0m | \u001b[0m 0.8977  \u001b[0m | \u001b[0m 0.4818  \u001b[0m | \u001b[0m 4.357e+0\u001b[0m | \u001b[0m 29.86   \u001b[0m | \u001b[0m 1.625   \u001b[0m | \u001b[0m 349.8   \u001b[0m | \u001b[0m 0.3607  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 159474\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 76.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.00495 \u001b[0m | \u001b[0m 0.6628  \u001b[0m | \u001b[0m 0.7135  \u001b[0m | \u001b[0m 0.1962  \u001b[0m | \u001b[0m 0.4965  \u001b[0m | \u001b[0m 0.6729  \u001b[0m | \u001b[0m 4.631e+0\u001b[0m | \u001b[0m 135.6   \u001b[0m | \u001b[0m 6.021   \u001b[0m | \u001b[0m 547.3   \u001b[0m | \u001b[0m 0.7127  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 189159\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1538  \u001b[0m | \u001b[0m 0.1666  \u001b[0m | \u001b[0m 0.4393  \u001b[0m | \u001b[0m 0.4137  \u001b[0m | \u001b[0m 0.5208  \u001b[0m | \u001b[0m 1.657e+0\u001b[0m | \u001b[0m 470.2   \u001b[0m | \u001b[0m 5.143   \u001b[0m | \u001b[0m 55.49   \u001b[0m | \u001b[0m 0.9341  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 84295\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 83.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.7902  \u001b[0m | \u001b[0m 0.5966  \u001b[0m | \u001b[0m 0.06417 \u001b[0m | \u001b[0m 0.109   \u001b[0m | \u001b[0m 0.9224  \u001b[0m | \u001b[0m 4.576e+0\u001b[0m | \u001b[0m 370.8   \u001b[0m | \u001b[0m 7.481   \u001b[0m | \u001b[0m 160.8   \u001b[0m | \u001b[0m 0.8326  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 107107\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 83.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 13.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.8882  \u001b[0m | \u001b[0m 0.1699  \u001b[0m | \u001b[0m 0.3794  \u001b[0m | \u001b[0m 0.6916  \u001b[0m | \u001b[0m 0.3344  \u001b[0m | \u001b[0m 4.788e+0\u001b[0m | \u001b[0m 478.1   \u001b[0m | \u001b[0m 3.979   \u001b[0m | \u001b[0m 318.1   \u001b[0m | \u001b[0m 0.7158  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 110666\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 81.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.6746  \u001b[0m | \u001b[0m 0.2052  \u001b[0m | \u001b[0m 0.1079  \u001b[0m | \u001b[0m 0.7933  \u001b[0m | \u001b[0m 0.9685  \u001b[0m | \u001b[0m 2.963e+0\u001b[0m | \u001b[0m 173.5   \u001b[0m | \u001b[0m 9.554   \u001b[0m | \u001b[0m 19.18   \u001b[0m | \u001b[0m 0.9854  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:51<00:00, 17.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 129324\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 77.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.05198 \u001b[0m | \u001b[0m 0.8067  \u001b[0m | \u001b[0m 0.7821  \u001b[0m | \u001b[0m 0.458   \u001b[0m | \u001b[0m 0.4187  \u001b[0m | \u001b[0m 0.9071  \u001b[0m | \u001b[0m 5.361e+0\u001b[0m | \u001b[0m 427.0   \u001b[0m | \u001b[0m 2.358   \u001b[0m | \u001b[0m 888.8   \u001b[0m | \u001b[0m 0.2965  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 234145\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 93.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:37<00:00, 12.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.02475 \u001b[0m | \u001b[0m 0.08825 \u001b[0m | \u001b[0m 0.9301  \u001b[0m | \u001b[0m 0.1988  \u001b[0m | \u001b[0m 0.2616  \u001b[0m | \u001b[0m 0.08231 \u001b[0m | \u001b[0m 1.906e+0\u001b[0m | \u001b[0m 272.8   \u001b[0m | \u001b[0m 4.253   \u001b[0m | \u001b[0m 292.7   \u001b[0m | \u001b[0m 0.2534  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:48<00:00, 16.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 138074\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 92.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1981  \u001b[0m | \u001b[0m 0.45    \u001b[0m | \u001b[0m 0.3096  \u001b[0m | \u001b[0m 0.2172  \u001b[0m | \u001b[0m 0.3631  \u001b[0m | \u001b[0m 3.789e+0\u001b[0m | \u001b[0m 11.04   \u001b[0m | \u001b[0m 7.341   \u001b[0m | \u001b[0m 602.8   \u001b[0m | \u001b[0m 0.8329  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 159522\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.2146  \u001b[0m | \u001b[0m 0.8475  \u001b[0m | \u001b[0m 0.136   \u001b[0m | \u001b[0m 0.8435  \u001b[0m | \u001b[0m 0.8077  \u001b[0m | \u001b[0m 3.112e+0\u001b[0m | \u001b[0m 399.2   \u001b[0m | \u001b[0m 6.117   \u001b[0m | \u001b[0m 557.8   \u001b[0m | \u001b[0m 0.749   \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:51<00:00, 17.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 316958\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 77.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.2954  \u001b[0m | \u001b[0m 0.1559  \u001b[0m | \u001b[0m 0.2124  \u001b[0m | \u001b[0m 0.3207  \u001b[0m | \u001b[0m 0.164   \u001b[0m | \u001b[0m 3.172e+0\u001b[0m | \u001b[0m 473.7   \u001b[0m | \u001b[0m 3.028   \u001b[0m | \u001b[0m 967.4   \u001b[0m | \u001b[0m 0.6826  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:51<00:00, 17.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 471390\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 81.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.04455 \u001b[0m | \u001b[0m 0.7641  \u001b[0m | \u001b[0m 0.9773  \u001b[0m | \u001b[0m 0.0623  \u001b[0m | \u001b[0m 0.3334  \u001b[0m | \u001b[0m 0.02887 \u001b[0m | \u001b[0m 3.987e+0\u001b[0m | \u001b[0m 187.0   \u001b[0m | \u001b[0m 2.003   \u001b[0m | \u001b[0m 481.8   \u001b[0m | \u001b[0m 0.3117  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 319077\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 68.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.0396  \u001b[0m | \u001b[0m 0.8617  \u001b[0m | \u001b[0m 0.8548  \u001b[0m | \u001b[0m 0.1384  \u001b[0m | \u001b[0m 0.5048  \u001b[0m | \u001b[0m 0.6808  \u001b[0m | \u001b[0m 1.846e+0\u001b[0m | \u001b[0m 197.5   \u001b[0m | \u001b[0m 1.251   \u001b[0m | \u001b[0m 317.5   \u001b[0m | \u001b[0m 0.2633  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 76275\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 80.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.6741  \u001b[0m | \u001b[0m 0.003397\u001b[0m | \u001b[0m 0.4969  \u001b[0m | \u001b[0m 0.6389  \u001b[0m | \u001b[0m 0.8443  \u001b[0m | \u001b[0m 2.543e+0\u001b[0m | \u001b[0m 348.1   \u001b[0m | \u001b[0m 1.918   \u001b[0m | \u001b[0m 421.1   \u001b[0m | \u001b[0m 0.2871  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 478969\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 75.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1394  \u001b[0m | \u001b[0m 0.7647  \u001b[0m | \u001b[0m 0.4374  \u001b[0m | \u001b[0m 0.669   \u001b[0m | \u001b[0m 0.07443 \u001b[0m | \u001b[0m 3.461e+0\u001b[0m | \u001b[0m 40.89   \u001b[0m | \u001b[0m 1.896   \u001b[0m | \u001b[0m 810.3   \u001b[0m | \u001b[0m 0.945   \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 110621\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 85.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.02475 \u001b[0m | \u001b[0m 0.6203  \u001b[0m | \u001b[0m 0.04108 \u001b[0m | \u001b[0m 0.3582  \u001b[0m | \u001b[0m 0.3993  \u001b[0m | \u001b[0m 0.8041  \u001b[0m | \u001b[0m 2.696e+0\u001b[0m | \u001b[0m 170.5   \u001b[0m | \u001b[0m 9.991   \u001b[0m | \u001b[0m 181.3   \u001b[0m | \u001b[0m 0.1334  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 465704\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 76.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.2649  \u001b[0m | \u001b[0m 0.8234  \u001b[0m | \u001b[0m 0.3501  \u001b[0m | \u001b[0m 0.3221  \u001b[0m | \u001b[0m 0.1402  \u001b[0m | \u001b[0m 3.323e+0\u001b[0m | \u001b[0m 245.0   \u001b[0m | \u001b[0m 1.222   \u001b[0m | \u001b[0m 49.62   \u001b[0m | \u001b[0m 0.9507  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 230611\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 80.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.02723 \u001b[0m | \u001b[0m 0.1782  \u001b[0m | \u001b[0m 0.1706  \u001b[0m | \u001b[0m 0.2208  \u001b[0m | \u001b[0m 0.9025  \u001b[0m | \u001b[0m 0.4369  \u001b[0m | \u001b[0m 1.712e+0\u001b[0m | \u001b[0m 196.4   \u001b[0m | \u001b[0m 3.794   \u001b[0m | \u001b[0m 627.2   \u001b[0m | \u001b[0m 0.2098  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 475403\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 71.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.03927 \u001b[0m | \u001b[0m 0.03917 \u001b[0m | \u001b[0m 0.1455  \u001b[0m | \u001b[0m 0.7006  \u001b[0m | \u001b[0m 0.4297  \u001b[0m | \u001b[0m 4e+05   \u001b[0m | \u001b[0m 311.2   \u001b[0m | \u001b[0m 2.579   \u001b[0m | \u001b[0m 128.5   \u001b[0m | \u001b[0m 0.6799  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 321193\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 75.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.7927  \u001b[0m | \u001b[0m 0.5291  \u001b[0m | \u001b[0m 0.2224  \u001b[0m | \u001b[0m 0.1856  \u001b[0m | \u001b[0m 0.5609  \u001b[0m | \u001b[0m 1.868e+0\u001b[0m | \u001b[0m 236.8   \u001b[0m | \u001b[0m 1.454   \u001b[0m | \u001b[0m 396.4   \u001b[0m | \u001b[0m 0.8897  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 168275\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 74.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1661  \u001b[0m | \u001b[0m 0.5151  \u001b[0m | \u001b[0m 0.2667  \u001b[0m | \u001b[0m 0.6265  \u001b[0m | \u001b[0m 0.8121  \u001b[0m | \u001b[0m 1.087e+0\u001b[0m | \u001b[0m 124.1   \u001b[0m | \u001b[0m 3.179   \u001b[0m | \u001b[0m 371.9   \u001b[0m | \u001b[0m 0.9561  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:48<00:00, 16.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 141010\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 82.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.8632  \u001b[0m | \u001b[0m 0.9208  \u001b[0m | \u001b[0m 0.4081  \u001b[0m | \u001b[0m 0.8871  \u001b[0m | \u001b[0m 0.776   \u001b[0m | \u001b[0m 9.548e+0\u001b[0m | \u001b[0m 10.92   \u001b[0m | \u001b[0m 5.631   \u001b[0m | \u001b[0m 843.1   \u001b[0m | \u001b[0m 0.8757  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:48<00:00, 16.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 158117\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 90.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:37<00:00, 12.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.09281 \u001b[0m | \u001b[0m 0.3145  \u001b[0m | \u001b[0m 0.4876  \u001b[0m | \u001b[0m 0.7952  \u001b[0m | \u001b[0m 0.1015  \u001b[0m | \u001b[0m 2.648e+0\u001b[0m | \u001b[0m 91.01   \u001b[0m | \u001b[0m 6.371   \u001b[0m | \u001b[0m 762.6   \u001b[0m | \u001b[0m 0.4506  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:48<00:00, 16.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 110374\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 89.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1601  \u001b[0m | \u001b[0m 0.3176  \u001b[0m | \u001b[0m 0.4936  \u001b[0m | \u001b[0m 0.9909  \u001b[0m | \u001b[0m 0.4133  \u001b[0m | \u001b[0m 4.546e+0\u001b[0m | \u001b[0m 170.6   \u001b[0m | \u001b[0m 9.721   \u001b[0m | \u001b[0m 989.3   \u001b[0m | \u001b[0m 0.9143  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:48<00:00, 16.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 206448\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 92.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:36<00:00, 12.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.02228 \u001b[0m | \u001b[0m 0.7373  \u001b[0m | \u001b[0m 0.09588 \u001b[0m | \u001b[0m 0.2027  \u001b[0m | \u001b[0m 0.6909  \u001b[0m | \u001b[0m 0.04423 \u001b[0m | \u001b[0m 1.491e+0\u001b[0m | \u001b[0m 375.6   \u001b[0m | \u001b[0m 3.033   \u001b[0m | \u001b[0m 240.9   \u001b[0m | \u001b[0m 0.1485  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 109841\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 91.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.04208 \u001b[0m | \u001b[0m 0.08419 \u001b[0m | \u001b[0m 0.764   \u001b[0m | \u001b[0m 0.4633  \u001b[0m | \u001b[0m 0.8293  \u001b[0m | \u001b[0m 0.1968  \u001b[0m | \u001b[0m 1.139e+0\u001b[0m | \u001b[0m 339.2   \u001b[0m | \u001b[0m 9.754   \u001b[0m | \u001b[0m 21.19   \u001b[0m | \u001b[0m 0.1167  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 263113\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 71.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.007426\u001b[0m | \u001b[0m 0.3552  \u001b[0m | \u001b[0m 0.1485  \u001b[0m | \u001b[0m 0.3696  \u001b[0m | \u001b[0m 0.4321  \u001b[0m | \u001b[0m 0.7391  \u001b[0m | \u001b[0m 2.035e+0\u001b[0m | \u001b[0m 18.29   \u001b[0m | \u001b[0m 3.987   \u001b[0m | \u001b[0m 211.9   \u001b[0m | \u001b[0m 0.3539  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 188726\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 82.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.3174  \u001b[0m | \u001b[0m 0.2146  \u001b[0m | \u001b[0m 0.8411  \u001b[0m | \u001b[0m 0.2534  \u001b[0m | \u001b[0m 1.461e+0\u001b[0m | \u001b[0m 450.7   \u001b[0m | \u001b[0m 5.79    \u001b[0m | \u001b[0m 59.1    \u001b[0m | \u001b[0m 0.4154  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 573370\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 66.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.009901\u001b[0m | \u001b[0m 0.4473  \u001b[0m | \u001b[0m 0.4131  \u001b[0m | \u001b[0m 0.1799  \u001b[0m | \u001b[0m 0.9116  \u001b[0m | \u001b[0m 0.6209  \u001b[0m | \u001b[0m 4.389e+0\u001b[0m | \u001b[0m 308.7   \u001b[0m | \u001b[0m 1.97    \u001b[0m | \u001b[0m 653.7   \u001b[0m | \u001b[0m 0.3336  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 159027\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 79.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.007426\u001b[0m | \u001b[0m 0.1796  \u001b[0m | \u001b[0m 0.8726  \u001b[0m | \u001b[0m 0.3895  \u001b[0m | \u001b[0m 0.5188  \u001b[0m | \u001b[0m 0.2826  \u001b[0m | \u001b[0m 1.928e+0\u001b[0m | \u001b[0m 376.9   \u001b[0m | \u001b[0m 6.617   \u001b[0m | \u001b[0m 895.5   \u001b[0m | \u001b[0m 0.6431  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:48<00:00, 16.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 110636\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 75.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.6208  \u001b[0m | \u001b[0m 0.7429  \u001b[0m | \u001b[0m 0.1442  \u001b[0m | \u001b[0m 0.09122 \u001b[0m | \u001b[0m 0.8526  \u001b[0m | \u001b[0m 2.491e+0\u001b[0m | \u001b[0m 461.3   \u001b[0m | \u001b[0m 9.017   \u001b[0m | \u001b[0m 647.7   \u001b[0m | \u001b[0m 0.975   \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 40746\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 91.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:37<00:00, 12.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.00495 \u001b[0m | \u001b[0m 0.01786 \u001b[0m | \u001b[0m 0.1247  \u001b[0m | \u001b[0m 0.3706  \u001b[0m | \u001b[0m 0.1153  \u001b[0m | \u001b[0m 0.3064  \u001b[0m | \u001b[0m 1.358e+0\u001b[0m | \u001b[0m 17.24   \u001b[0m | \u001b[0m 5.309   \u001b[0m | \u001b[0m 186.8   \u001b[0m | \u001b[0m 0.1863  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 159100\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 80.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.01733 \u001b[0m | \u001b[0m 0.3722  \u001b[0m | \u001b[0m 0.3996  \u001b[0m | \u001b[0m 0.2777  \u001b[0m | \u001b[0m 0.4973  \u001b[0m | \u001b[0m 0.3127  \u001b[0m | \u001b[0m 1.837e+0\u001b[0m | \u001b[0m 389.0   \u001b[0m | \u001b[0m 6.065   \u001b[0m | \u001b[0m 754.9   \u001b[0m | \u001b[0m 0.3064  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 122259\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.6718  \u001b[0m | \u001b[0m 0.6237  \u001b[0m | \u001b[0m 0.3085  \u001b[0m | \u001b[0m 0.3622  \u001b[0m | \u001b[0m 0.2712  \u001b[0m | \u001b[0m 1.302e+0\u001b[0m | \u001b[0m 468.9   \u001b[0m | \u001b[0m 8.643   \u001b[0m | \u001b[0m 1.014e+0\u001b[0m | \u001b[0m 0.9802  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 159345\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 81.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.8243  \u001b[0m | \u001b[0m 0.8727  \u001b[0m | \u001b[0m 0.2024  \u001b[0m | \u001b[0m 0.1704  \u001b[0m | \u001b[0m 0.4748  \u001b[0m | \u001b[0m 3.046e+0\u001b[0m | \u001b[0m 488.1   \u001b[0m | \u001b[0m 6.923   \u001b[0m | \u001b[0m 296.8   \u001b[0m | \u001b[0m 0.8762  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 99492\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 75.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.7691  \u001b[0m | \u001b[0m 0.05714 \u001b[0m | \u001b[0m 0.01803 \u001b[0m | \u001b[0m 0.4008  \u001b[0m | \u001b[0m 0.4224  \u001b[0m | \u001b[0m 5.418e+0\u001b[0m | \u001b[0m 157.5   \u001b[0m | \u001b[0m 5.342   \u001b[0m | \u001b[0m 930.7   \u001b[0m | \u001b[0m 0.8257  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:47<00:00, 15.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 138298\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 79.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 67      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.4083  \u001b[0m | \u001b[0m 0.619   \u001b[0m | \u001b[0m 0.3114  \u001b[0m | \u001b[0m 0.3908  \u001b[0m | \u001b[0m 0.5529  \u001b[0m | \u001b[0m 1.66e+05\u001b[0m | \u001b[0m 45.37   \u001b[0m | \u001b[0m 7.676   \u001b[0m | \u001b[0m 680.0   \u001b[0m | \u001b[0m 0.8261  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 122310\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 91.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 0.02723 \u001b[0m | \u001b[0m 0.1816  \u001b[0m | \u001b[0m 0.572   \u001b[0m | \u001b[0m 0.04061 \u001b[0m | \u001b[0m 0.1067  \u001b[0m | \u001b[0m 0.2911  \u001b[0m | \u001b[0m 2.115e+0\u001b[0m | \u001b[0m 227.7   \u001b[0m | \u001b[0m 8.381   \u001b[0m | \u001b[0m 254.4   \u001b[0m | \u001b[0m 0.2546  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:48<00:00, 16.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 317934\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 77.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:41<00:00, 13.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 0.04703 \u001b[0m | \u001b[0m 0.5935  \u001b[0m | \u001b[0m 0.2127  \u001b[0m | \u001b[0m 0.1782  \u001b[0m | \u001b[0m 0.3913  \u001b[0m | \u001b[0m 0.8924  \u001b[0m | \u001b[0m 3.46e+05\u001b[0m | \u001b[0m 415.5   \u001b[0m | \u001b[0m 3.568   \u001b[0m | \u001b[0m 146.7   \u001b[0m | \u001b[0m 0.1656  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:51<00:00, 17.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 235633\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 73.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 70      \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.5439  \u001b[0m | \u001b[0m 0.2183  \u001b[0m | \u001b[0m 0.4356  \u001b[0m | \u001b[0m 0.3447  \u001b[0m | \u001b[0m 0.5501  \u001b[0m | \u001b[0m 2.398e+0\u001b[0m | \u001b[0m 307.9   \u001b[0m | \u001b[0m 4.328   \u001b[0m | \u001b[0m 635.9   \u001b[0m | \u001b[0m 0.5434  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 189063\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 80.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 71      \u001b[0m | \u001b[0m 0.009901\u001b[0m | \u001b[0m 0.1368  \u001b[0m | \u001b[0m 0.512   \u001b[0m | \u001b[0m 0.04574 \u001b[0m | \u001b[0m 0.3439  \u001b[0m | \u001b[0m 0.4284  \u001b[0m | \u001b[0m 3.919e+0\u001b[0m | \u001b[0m 231.1   \u001b[0m | \u001b[0m 5.491   \u001b[0m | \u001b[0m 941.7   \u001b[0m | \u001b[0m 0.3282  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:50<00:00, 16.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 317744\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 72      \u001b[0m | \u001b[95m 0.07178 \u001b[0m | \u001b[95m 0.603   \u001b[0m | \u001b[95m 0.8462  \u001b[0m | \u001b[95m 0.4238  \u001b[0m | \u001b[95m 0.963   \u001b[0m | \u001b[95m 0.3563  \u001b[0m | \u001b[95m 2.424e+0\u001b[0m | \u001b[95m 6.153   \u001b[0m | \u001b[95m 2.856   \u001b[0m | \u001b[95m 431.7   \u001b[0m | \u001b[95m 0.131   \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 168684\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 82.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.9559  \u001b[0m | \u001b[0m 0.9824  \u001b[0m | \u001b[0m 0.2533  \u001b[0m | \u001b[0m 0.5811  \u001b[0m | \u001b[0m 0.9323  \u001b[0m | \u001b[0m 1.174e+0\u001b[0m | \u001b[0m 170.5   \u001b[0m | \u001b[0m 4.899   \u001b[0m | \u001b[0m 224.6   \u001b[0m | \u001b[0m 0.9569  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 189256\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 71.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5296  \u001b[0m | \u001b[0m 0.1281  \u001b[0m | \u001b[0m 0.3233  \u001b[0m | \u001b[0m 0.9105  \u001b[0m | \u001b[0m 0.6901  \u001b[0m | \u001b[0m 4.204e+0\u001b[0m | \u001b[0m 75.85   \u001b[0m | \u001b[0m 5.614   \u001b[0m | \u001b[0m 415.2   \u001b[0m | \u001b[0m 0.6169  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 198150\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 70.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 75      \u001b[0m | \u001b[0m 0.007426\u001b[0m | \u001b[0m 0.5316  \u001b[0m | \u001b[0m 0.4871  \u001b[0m | \u001b[0m 0.1554  \u001b[0m | \u001b[0m 0.1681  \u001b[0m | \u001b[0m 0.8707  \u001b[0m | \u001b[0m 1.385e+0\u001b[0m | \u001b[0m 110.6   \u001b[0m | \u001b[0m 3.298   \u001b[0m | \u001b[0m 993.5   \u001b[0m | \u001b[0m 0.4419  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 264474\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 75.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.9736  \u001b[0m | \u001b[0m 0.3016  \u001b[0m | \u001b[0m 0.3128  \u001b[0m | \u001b[0m 0.4889  \u001b[0m | \u001b[0m 0.7939  \u001b[0m | \u001b[0m 2.049e+0\u001b[0m | \u001b[0m 125.0   \u001b[0m | \u001b[0m 3.422   \u001b[0m | \u001b[0m 416.9   \u001b[0m | \u001b[0m 0.6387  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 137988\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 87.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5787  \u001b[0m | \u001b[0m 0.02431 \u001b[0m | \u001b[0m 0.1555  \u001b[0m | \u001b[0m 0.9569  \u001b[0m | \u001b[0m 0.3102  \u001b[0m | \u001b[0m 1.853e+0\u001b[0m | \u001b[0m 409.6   \u001b[0m | \u001b[0m 7.909   \u001b[0m | \u001b[0m 628.0   \u001b[0m | \u001b[0m 0.5564  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 138349\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 82.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.07075 \u001b[0m | \u001b[0m 0.2582  \u001b[0m | \u001b[0m 0.4281  \u001b[0m | \u001b[0m 0.1925  \u001b[0m | \u001b[0m 0.6416  \u001b[0m | \u001b[0m 4.993e+0\u001b[0m | \u001b[0m 322.1   \u001b[0m | \u001b[0m 7.174   \u001b[0m | \u001b[0m 324.9   \u001b[0m | \u001b[0m 0.8769  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 431674\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 74.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 0.0198  \u001b[0m | \u001b[0m 0.4337  \u001b[0m | \u001b[0m 0.7745  \u001b[0m | \u001b[0m 0.3136  \u001b[0m | \u001b[0m 0.5936  \u001b[0m | \u001b[0m 0.1095  \u001b[0m | \u001b[0m 3.573e+0\u001b[0m | \u001b[0m 318.8   \u001b[0m | \u001b[0m 2.49    \u001b[0m | \u001b[0m 427.5   \u001b[0m | \u001b[0m 0.3271  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 470568\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 79.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 0.002475\u001b[0m | \u001b[0m 0.7486  \u001b[0m | \u001b[0m 0.3043  \u001b[0m | \u001b[0m 0.3527  \u001b[0m | \u001b[0m 0.02006 \u001b[0m | \u001b[0m 0.1318  \u001b[0m | \u001b[0m 3.96e+05\u001b[0m | \u001b[0m 23.98   \u001b[0m | \u001b[0m 2.588   \u001b[0m | \u001b[0m 1.021e+0\u001b[0m | \u001b[0m 0.5897  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:48<00:00, 16.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 107875\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 88.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:35<00:00, 11.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1207  \u001b[0m | \u001b[0m 0.1955  \u001b[0m | \u001b[0m 0.07372 \u001b[0m | \u001b[0m 0.1927  \u001b[0m | \u001b[0m 0.03631 \u001b[0m | \u001b[0m 1.295e+0\u001b[0m | \u001b[0m 461.5   \u001b[0m | \u001b[0m 9.989   \u001b[0m | \u001b[0m 43.65   \u001b[0m | \u001b[0m 0.5526  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 122828\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 77.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 82      \u001b[0m | \u001b[0m 0.04208 \u001b[0m | \u001b[0m 0.4501  \u001b[0m | \u001b[0m 0.5975  \u001b[0m | \u001b[0m 0.2442  \u001b[0m | \u001b[0m 0.5249  \u001b[0m | \u001b[0m 0.9538  \u001b[0m | \u001b[0m 4.484e+0\u001b[0m | \u001b[0m 486.9   \u001b[0m | \u001b[0m 8.003   \u001b[0m | \u001b[0m 734.8   \u001b[0m | \u001b[0m 0.2898  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 245118\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 74.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.4115  \u001b[0m | \u001b[0m 0.1216  \u001b[0m | \u001b[0m 0.3614  \u001b[0m | \u001b[0m 0.911   \u001b[0m | \u001b[0m 0.69    \u001b[0m | \u001b[0m 1.695e+0\u001b[0m | \u001b[0m 109.2   \u001b[0m | \u001b[0m 2.783   \u001b[0m | \u001b[0m 514.5   \u001b[0m | \u001b[0m 0.9592  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 120897\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 92.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:36<00:00, 12.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 84      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1725  \u001b[0m | \u001b[0m 0.07458 \u001b[0m | \u001b[0m 0.0874  \u001b[0m | \u001b[0m 0.07784 \u001b[0m | \u001b[0m 0.06502 \u001b[0m | \u001b[0m 1.712e+0\u001b[0m | \u001b[0m 205.3   \u001b[0m | \u001b[0m 8.855   \u001b[0m | \u001b[0m 615.2   \u001b[0m | \u001b[0m 0.2677  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:47<00:00, 15.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 377382\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 72.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:40<00:00, 13.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 85      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.8289  \u001b[0m | \u001b[0m 0.1422  \u001b[0m | \u001b[0m 0.3289  \u001b[0m | \u001b[0m 0.2713  \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 2.428e+0\u001b[0m | \u001b[0m 257.4   \u001b[0m | \u001b[0m 1.026   \u001b[0m | \u001b[0m 217.6   \u001b[0m | \u001b[0m 0.6509  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:49<00:00, 16.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 394465\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 74.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:39<00:00, 13.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 0.01733 \u001b[0m | \u001b[0m 0.04424 \u001b[0m | \u001b[0m 0.7572  \u001b[0m | \u001b[0m 0.03205 \u001b[0m | \u001b[0m 0.3565  \u001b[0m | \u001b[0m 0.4114  \u001b[0m | \u001b[0m 2.602e+0\u001b[0m | \u001b[0m 86.22   \u001b[0m | \u001b[0m 1.273   \u001b[0m | \u001b[0m 1.016e+0\u001b[0m | \u001b[0m 0.2043  \u001b[0m |\n",
      "Fitting vectorizers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  \"The parameter 'token_pattern' will not be used\"\n",
      "100%|██████████| 3/3 [00:48<00:00, 16.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 111186\n",
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 83.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.71s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(init_points = 10, n_iter = 100)\n",
    "print(\"Best result: {}; f(x) = {}.\".format(optimizer.max[\"params\"], optimizer.max[\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnr8rz7vidR8"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Au4fwI1jpsjg"
   },
   "outputs": [],
   "source": [
    "pifa_concat = LabelEmbeddingFactory.create(train_question_paper_csr, \n",
    "                                           train_questions_csr, \n",
    "                                           Z=papers_csr, \n",
    "                                           method=\"pifa_lf_concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "F3xZi7C2psjh"
   },
   "outputs": [],
   "source": [
    "cluster_chain = Indexer.gen(pifa_concat, \n",
    "                            indexer_type=\"hierarchicalkmeans\",\n",
    "                            nr_splits=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vWGbzGMS9Mcu"
   },
   "outputs": [],
   "source": [
    "xlm = XLinearModel.train(train_questions_csr, \n",
    "                         train_question_paper_csr, \n",
    "                         C=cluster_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xeoTWKpsJpj"
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "sEx8M2Mapsji"
   },
   "outputs": [],
   "source": [
    "predicted_question_paper_csr = xlm.predict(valid_questions_csr,\n",
    "                                           beam_size=100,\n",
    "                                           only_topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTYSOryS97z5",
    "outputId": "5ff1faae-b985-406b-bf98-17f72e6a2d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec   = 8.42 6.19 5.12 4.46 3.96 3.59 3.22 3.03 2.89 2.82\n",
      "recall = 3.06 3.78 4.10 4.48 4.64 4.86 4.97 5.13 5.33 5.61\n"
     ]
    }
   ],
   "source": [
    "metric = smat_util.Metrics.generate(valid_question_paper_csr, \n",
    "                                    predicted_question_paper_csr, \n",
    "                                    topk=10)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEP1i9B7mrNl"
   },
   "source": [
    "### Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KwaByceA--yx"
   },
   "outputs": [],
   "source": [
    "def get_papers_by_text(text, \n",
    "                       vectorizer,\n",
    "                       number_of_papers=10, \n",
    "                       check_correctness=False):\n",
    "      \n",
    "    print('Qestion: ', text)\n",
    "    vector_text_csr = vectorizer.transform([text])\n",
    "\n",
    "    papers_csr = xlm.predict(vector_text_csr,\n",
    "                             beam_size=4,\n",
    "                             only_topk=number_of_papers)\n",
    "\n",
    "    prediction = papers_csr.toarray()\n",
    "    paper_labels = np.nonzero(prediction)[1].tolist()\n",
    "\n",
    "    # print()\n",
    "    # print('Found papers: ', len(paper_labels))\n",
    "    # print(paper_labels)\n",
    "\n",
    "    paper_logists = np.take(prediction, paper_labels).tolist()\n",
    "\n",
    "    result = papers_df.loc[paper_labels, ['title', 'abstract', 'snippet_url']]\n",
    "    result['score'] = paper_logists\n",
    "    result = result.sort_values('score', ascending=False)\n",
    "\n",
    "    if check_correctness:\n",
    "        correct_papers = df.loc[df.question_text == text, 'paper_id'].unique()\n",
    "        print(f'\\nTrue number of papers: {len(correct_papers)}')\n",
    "        result['correctness'] = result.index.isin(correct_papers)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gwbj2ftqiDCq"
   },
   "outputs": [],
   "source": [
    "correct_examples = [\n",
    "    'Which genome browser database for DNA shape annotations is available?',\n",
    "    'Has strimvelis been approved by the European Medicines Agency?',\n",
    "    'What is the target of Volanesorsen?',\n",
    "    'Is eptinezumab a small molecule?'\n",
    "]\n",
    "\n",
    "interesting_examples = [\n",
    "    'For which type of diabetes can empagliflozin be used?',\n",
    "    \"Is Semagacestat effective for Alzheimer's Disease?\",\n",
    "    'Which gene is associated with Muenke syndrome?'     \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mJFJaoQFkPSP",
    "outputId": "58546ec6-8979-454d-de9d-e5669b4723dd"
   },
   "outputs": [],
   "source": [
    "get_papers_by_text('Which gene is associated with Muenke syndrome?',\n",
    "                   vectorizer=vectorizer, \n",
    "                   number_of_papers=30,\n",
    "                   check_correctness=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MPSFMFtZg73j",
    "outputId": "b5bb4cfd-0c4f-4b1e-92e1-440830e04da6"
   },
   "outputs": [],
   "source": [
    "# Random example\n",
    "temp = valid_questions_df.question_text.sample().values[0]\n",
    "print(temp)\n",
    "\n",
    "get_papers_by_text(temp,\n",
    "                   vectorizer=vectorizer,\n",
    "                   number_of_papers=30,\n",
    "                   check_correctness=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJFYVPRcXeuu",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Highlight answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFpTYj2txYtl"
   },
   "outputs": [],
   "source": [
    "question_text = 'Which genome browser database for DNA shape annotations is available?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUUo8G9lxuny",
    "outputId": "0e84819c-6550-44f8-b75b-0de6bfe71860"
   },
   "outputs": [],
   "source": [
    "result = get_papers_by_text(\n",
    "    question_text,\n",
    "    vectorizer=vectorizer, \n",
    "    number_of_papers=30,\n",
    "    check_correctness=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdyBfWuaXj_0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"ozcangundes/T5-base-for-BioQA\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ozcangundes/T5-base-for-BioQA\")\n",
    "\n",
    "def get_answer(question, context):\n",
    "    source_encoding=tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=\"only_second\",\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\")\n",
    "\n",
    "    generated_ids=model.generate(\n",
    "        input_ids=source_encoding[\"input_ids\"],\n",
    "        attention_mask=source_encoding[\"attention_mask\"])\n",
    "\n",
    "    preds=[tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for gen_id in generated_ids]\n",
    "    \n",
    "    return \"\".join(preds)\n",
    "\n",
    "# To highlight the answers in the results\n",
    "def highlight_selected_text(row):\n",
    "    abstract = row[\"abstract\"]\n",
    "    title = row[\"title\"]\n",
    "    ext = []\n",
    "    ext.append(row[\"abstract_answer\"])\n",
    "    ext.append(row['title_answer'])\n",
    "    # for k, v in color.items():\n",
    "    for k in ext:\n",
    "      if len(k) > 2:\n",
    "        abstract = abstract.replace(k, f'<span style=\"color: red; background-color: yellow; font-weight: bold\">{k}</span>')\n",
    "        title = title.replace(k, f'<span style=\"color: red; background-color: yellow; font-weight: bold\">{k}</span>')\n",
    "    return abstract, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrPZWXcCtYf8"
   },
   "outputs": [],
   "source": [
    "result['abstract_answer'] = result['abstract'].apply(lambda x: get_answer(question_text, x))\n",
    "result['title_answer'] = result['title'].apply(lambda x: get_answer(question_text, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wl-Fa-FQtcz3",
    "outputId": "052f5830-3859-4be7-8da0-633ec86b6923"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "result['abstract'], result['title'] = zip(*result.apply(highlight_selected_text, axis=1))\n",
    "display(HTML(result.sample(10).to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rY3e57CzaVP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CEGFQWTOeGvN",
    "vGIJYIiReK5d",
    "rNc4msLBeXJ_",
    "ejYlTf03psjW",
    "Ab-ejPbxpsjb",
    "1g9IMdg7psjd",
    "dvUY8ooMQ-Rn",
    "fnr8rz7vidR8"
   ],
   "name": "XMC.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
